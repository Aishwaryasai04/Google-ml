
LOGISTIC REGRESSION:
Logistic regression is designed to predict the probability of a given outcome.
For example: Is this email spam? Will it rain today?
It uses sigmoid function(s-shaped) has a formula
F(x)=1/(1+e-x)


 

Graph of the sigmoid function. The curve approaches 0 as x values decrease to negative infinity, and 1 as x values increase toward infinity

Transforming linear output using the sigmoid function
The following equation represents the linear component of a logistic regression model:
z=b+w1x1+w2x2+…+wNxN
where:
•	z is the output of the linear equation, also called the log odds.
•	b is the bias.
•	The w values are the model's learned weights.
•	The x values are the feature values for a particular example.
To obtain the logistic regression prediction, the z value is then passed to the sigmoid function, yielding a value (a probability) between 0 and 1:
y′=11+e−z
where:
•	y' is the output of the logistic regression model.
•	z is the linear output (as calculated in the preceding equation).

Logistic regression uses the same process as linear regression with two distinctive points:
1 Logistic uses log loss as the loss function instead of squared loss.
2 Applying regularization is critical to prevent overfitting.
Log Loss: 
It returns the logarithm of the magnitude of the change rather than just the distance from data to prediction.
It is calculated as:
Log Loss=∑(x,y)∈D−ylog⁡(y′)−(1−y)log⁡(1−y′)
where:
•	(x,y)∈D is the dataset containing many labeled examples, which are (x,y) pairs.
•	y is the label in a labeled example. Since this is logistic regression, every value of y must either be 0 or 1.
•	y′ is your model's prediction (somewhere between 0 and 1), given the set of features in x.

Regularization in logistic regression:

It is a mechanism for penalizing model complexity during training, is extremely important in logistic regression modeling. Without regularization, the asymptotic nature of logistic regression would keep driving loss towards 0 in cases where the model has a large number of features. Consequently, most logistic regression models use one of the following two strategies to decrease model complexity.

L2 regularization
Early stopping: limiting the number of training steps to halt training while loss is still decreasing.
